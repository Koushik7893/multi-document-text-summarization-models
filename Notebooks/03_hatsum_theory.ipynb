{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a74684",
   "metadata": {},
   "source": [
    "Got it, Koushik! Here's a **complete, minimal example** to get you started with **HAT-Sum (Hierarchical Transformers for Multi-Document Summarization)** \u2014 focused on training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "# \ud83d\udd25 HAT-Sum: Full Code Example (Simplified)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Clone & Setup**\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/psunlpgroup/HATSum.git\n",
    "cd HATSum\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Download & Prepare Dataset**\n",
    "\n",
    "The recommended dataset is **WikiSum (ranked paragraphs)**:\n",
    "\n",
    "* Download dataset from the official [HATSum WikiSum link](https://github.com/psunlpgroup/HATSum#datasets) or via Google Drive in the repo.\n",
    "* Unpack and note the path, say `DATA_DIR`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Training Script**\n",
    "\n",
    "This runs the hierarchical model training on WikiSum:\n",
    "\n",
    "```bash\n",
    "python train_abstractive.py \\\n",
    "  -mode train \\\n",
    "  -data_path DATA_DIR \\\n",
    "  -hier \\\n",
    "  -batch_size 8 \\\n",
    "  -train_steps 200000 \\\n",
    "  -inter_layers 6,7 \\\n",
    "  -inter_heads 8 \\\n",
    "  -vocab_path DATA_DIR/vocab \\\n",
    "  -model_path OUTPUT_DIR/\n",
    "```\n",
    "\n",
    "* `-hier` enables hierarchical encoding\n",
    "* `-inter_layers` and `-inter_heads` configure the global Transformer encoder\n",
    "* Adjust `train_steps` & `batch_size` per your compute\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Validation / Evaluation**\n",
    "\n",
    "```bash\n",
    "python train_abstractive.py \\\n",
    "  -mode validate \\\n",
    "  -hier \\\n",
    "  -data_path DATA_DIR \\\n",
    "  -model_path OUTPUT_DIR/ \\\n",
    "  -report_rouge\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Generate Summaries (Testing)**\n",
    "\n",
    "```bash\n",
    "python train_abstractive.py \\\n",
    "  -mode test \\\n",
    "  -hier \\\n",
    "  -data_path DATA_DIR \\\n",
    "  -model_path OUTPUT_DIR/ \\\n",
    "  -max_wiki 100000 \\\n",
    "  -trunc_tgt_ntoken 400\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Key Python File: `train_abstractive.py` Overview**\n",
    "\n",
    "* It loads hierarchical data (multi-paragraph, multi-doc)\n",
    "* Encodes paragraphs with a local encoder (Transformer or BERT)\n",
    "* Uses a global Transformer encoder to merge context\n",
    "* Transformer decoder generates abstractive summaries\n",
    "* Implements ROUGE during validation\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Sample Snippet to Load & Use Model**\n",
    "\n",
    "```python\n",
    "from model import HierarchicalTransformer  # hypothetical import\n",
    "from dataset import WikiSumDataset\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = WikiSumDataset(data_path='DATA_DIR', mode='train', hierarchical=True)\n",
    "\n",
    "# Initialize model\n",
    "model = HierarchicalTransformer(\n",
    "    local_encoder_layers=6,\n",
    "    global_encoder_layers=2,\n",
    "    decoder_layers=6,\n",
    "    heads=8,\n",
    "    vocab_size=train_dataset.vocab_size\n",
    ")\n",
    "\n",
    "# Training loop (simplified)\n",
    "for batch in train_dataset.get_batches(batch_size=8):\n",
    "    inputs, targets = batch['inputs'], batch['targets']\n",
    "    outputs = model(inputs)\n",
    "    loss = compute_loss(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "* This repo is **research-level code**, so some understanding of PyTorch and Transformers is required.\n",
    "* For full usage, check the repo's README and scripts.\n",
    "* You can adapt training hyperparameters for your GPU limits.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can help you:\n",
    "\n",
    "* Walk through **key functions** inside `train_abstractive.py`\n",
    "* Write a **simple notebook example** to run a smaller demo\n",
    "* Show how to **fine-tune or infer** with your own multi-doc inputs\n",
    "\n",
    "Just say the word!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}